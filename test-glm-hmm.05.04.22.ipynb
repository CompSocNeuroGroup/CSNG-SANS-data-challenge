{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Mar 31 11:19:36 2022\n",
    "@author: jthompsz\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('PS') \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel as nib\n",
    "import nilearn as ni\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "from fnl_tools.stats import hmm_bic\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "npr.seed(0)\n",
    "\n",
    "import ssm\n",
    "from ssm.util import find_permutation\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "color_names = [\n",
    "    \"windows blue\",\n",
    "    \"red\",\n",
    "    \"amber\",\n",
    "    \"faded green\",\n",
    "    \"dusty purple\",\n",
    "    \"orange\"\n",
    "    ]\n",
    "\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "cmap = gradient_cmap(colors)\n",
    "\n",
    "base_dir = '/mnt/EE9A47C59A478953/data/FNL'\n",
    "out_dir = '/mnt/EE9A47C59A478953/data/FNL/output'\n",
    "func_data = '/data/test'\n",
    "\n",
    "\n",
    "os.chdir(f'{base_dir}')\n",
    "\n",
    "# load in target ROI\n",
    "target_roi = image.load_img(f'{base_dir}/ROIs/vmpfc-chang.nii.gz')\n",
    "plotting.plot_roi(target_roi)\n",
    "\n",
    "# load source ROIs\n",
    "atlas = image.load_img(f'{base_dir}/ROIs/source_rois.nii.gz')\n",
    "\n",
    "# load in some data\n",
    "sdata = image.load_img(f'{base_dir}/{func_data}/sub-sid000216_task-movie_run-1_space-MNI152NLin2009cAsym_desc-preproc_trim_smooth6_denoised_bold.nii.gz')\n",
    "\n",
    "# extract target data from roi\n",
    "target_masker = NiftiMasker(mask_img=target_roi, standardize=False)\n",
    "target_time_series = target_masker.fit_transform(sdata)\n",
    "\n",
    "target_data = pd.DataFrame(target_time_series)\n",
    "\n",
    "\n",
    "# extract source data from rois\n",
    "masker = NiftiLabelsMasker(labels_img=atlas, standardize=False)\n",
    "source_time_series = masker.fit_transform(sdata)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(zscore(source_time_series[:,0]))\n",
    "plt.show()\n",
    "\n",
    "#####                HMM as in Chang et al\n",
    "\n",
    "# Reduce Data Dimensionality\n",
    "target_var = 0.9\n",
    "pca = PCA(n_components=target_var)\n",
    "reduced = pca.fit_transform(zscore(target_data))\n",
    "\n",
    "# Run HMM\n",
    "k=4\n",
    "m1 = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", algorithm='map', n_iter=500)\n",
    "m1.fit(reduced)\n",
    "\n",
    "# Get HMM Weights\n",
    "w = pd.DataFrame(pca.inverse_transform(m1.means_))\n",
    "w.round(decimals=4).to_csv(os.path.join(out_dir, f'HMMWeights.vmpfc.k4.csv'))\n",
    "    \n",
    "# Write out HMM Covariance\n",
    "for i,x in enumerate(m1.covars_):\n",
    "    pd.DataFrame(x).to_csv(os.path.join(out_dir, f'HMMCovariates.vmpfc.k4.csv'))\n",
    "    \n",
    "# Write out Transition matrix\n",
    "transmat = pd.DataFrame(m1.transmat_)\n",
    "transmat.to_csv(os.path.join(out_dir,f'HMMTransitionMatrix.vmpfc.k4.csv'))\n",
    "    \n",
    "# Write out predicted states\n",
    "pred = {}\n",
    "p = m1.decode(reduced, algorithm='viterbi')\n",
    "pred['Viterbi'] = p[1]\n",
    "pred['MAP'] = m1.decode(reduced, algorithm='map')[1]\n",
    "pred_prob = m1.predict_proba(reduced)\n",
    "for i in range(k):\n",
    "        pred[f'Probability_{i}'] = pred_prob[:,i]\n",
    "        proj = np.dot(reduced, m1.means_.T)\n",
    "        for i in range(k):\n",
    "            pred[f'Projected_{i}'] = proj[:,i]\n",
    "            pred = pd.DataFrame(pred)\n",
    "            pred['ModelFit'] = m1.score(reduced)\n",
    "            #pred['Subject'] = sub\n",
    "            #pred['Study'] = study\n",
    "            pred['PCA_Components'] = reduced.shape[1]\n",
    "            pred.to_csv(os.path.join(out_dir,f'HMMPredictedStates.vmpfc.k{k}.csv'))\n",
    "\n",
    "#plt.figure(figsize=(10,3))\n",
    "#plt.plot(pred[['Probability_0','Probability_1','Probability_2','Probability_3' ]])\n",
    "#plt.show()\n",
    "\n",
    "# read in HMM data\n",
    "\n",
    "p = pd.read_csv(os.path.join(out_dir, f'HMMPredictedStates.vmpfc.k{k}.csv'), index_col=0)\n",
    "\n",
    "#####                HMM as in SSM\n",
    "\n",
    "N_iters = 500\n",
    "\n",
    "## testing the constrained transitions class\n",
    "hmm = ssm.HMM(k, 70, observations=\"diagonal_gaussian\")\n",
    "\n",
    "hmm_lls = hmm.fit(reduced, method=\"em\", num_iters=N_iters, init_method=\"kmeans\")\n",
    "\n",
    "hmm_z = hmm.most_likely_states(reduced)\n",
    "\n",
    "ssm_out = pd.DataFrame()\n",
    "ssm_out['zPredicted States'] = hmm_z\n",
    "\n",
    "ssm.to_csv(os.path.join(out_dir,f'SSM-HMMPredictedStates.vmpfc.k{k}.csv'))\n",
    "\n",
    "# align states for comparison - this is a dumb way to do it\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Viterbi'] = p['Viterbi']\n",
    "df['Viterbi'] = df['Viterbi'].replace(0, 'D')\n",
    "df['Viterbi'] = df['Viterbi'].replace(1, 'C')\n",
    "df['Viterbi'] = df['Viterbi'].replace(2, 'B')\n",
    "df['Viterbi'] = df['Viterbi'].replace(3, 'A')\n",
    "\n",
    "df['Viterbi'] = df['Viterbi'].replace('D',3)\n",
    "df['Viterbi'] = df['Viterbi'].replace('C',2)\n",
    "df['Viterbi'] = df['Viterbi'].replace('B',1)\n",
    "df['Viterbi'] = df['Viterbi'].replace('A',0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.subplot(311)\n",
    "plt.imshow(hmm_z[None,:], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "plt.ylabel(\"SSM - $z_{\\\\mathrm{inferred}}$\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.imshow([df['Viterbi']], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "plt.ylabel(\"HMM - Viterbi aligned\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "\n",
    "# unaligned\n",
    "plt.subplot(313)\n",
    "plt.imshow([p['Viterbi']], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "plt.ylabel(\"HMM - Viterbi\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "r = pearsonr(hmm_z,df['Viterbi'] )\n",
    "dist = hamming(hmm_z,df['Viterbi'] )\n",
    "\n",
    "\n",
    "fig.savefig('most_likely_states - aligned.png', dpi=300)\n",
    "\n",
    "#####               Input-driven HMM as in SSM\n",
    "N_iters = 500\n",
    "\n",
    "hmm2 = ssm.HMM(k, 70, 1, observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "\n",
    "# Fit\n",
    "a = (zscore(source_time_series[:,0]))\n",
    "a = a.reshape(len(a),1)\n",
    "hmm_lps = hmm2.fit(reduced, inputs=a, method=\"em\", num_iters=N_iters)\n",
    "\n",
    "hmm2_z = hmm2.most_likely_states(reduced)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.subplot(211)\n",
    "plt.imshow(hmm_z[None,:], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "plt.ylabel(\"SSM - $z_{\\\\mathrm{inferred}}$\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(hmm2_z[None,:], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "plt.ylabel(\"input driven SSM - $z_{\\\\mathrm{inferred}}$\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "fig.savefig('most_likely_states - w input driven.png', dpi=300)\n",
    "\n",
    "# Transition Matrices\n",
    "\n",
    "learned_transition_mat = hmm.transitions.transition_matrix\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(learned_transition_mat, cmap='bone', clim=(0.0, 0.2))\n",
    "plt.title(\"Learned Transition Matrix\")\n",
    "\n",
    "learned_transition_mat2 = hmm2.transitions.transition_matrix\n",
    "plt.subplot(122)\n",
    "im = plt.imshow(learned_transition_mat2, cmap='bone', clim=(0.0, 0.2))\n",
    "plt.title(\"Learned Transition Matrix\")\n",
    "\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# log likelihood - BIC or WAIC would be better\n",
    "mle_lls = hmm.log_likelihood(reduced)\n",
    "mle_lps = hmm2.log_likelihood(reduced, a)\n",
    "\n",
    "bic_lls = hmm_bic(LL=mle_lls, n_states=4, n_features=70)\n",
    "bic_lps = hmm_bic(LL=mle_lps, n_states=4, n_features=71)\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "loglikelihood_vals = [bic_lls, bic_lps]\n",
    "colors = ['Red', 'Purple']\n",
    "for z, occ in enumerate(loglikelihood_vals):\n",
    "    plt.bar(z, occ, width = 0.8, color = colors[z])\n",
    "plt.ylim([547000, 547405])\n",
    "plt.xticks([0, 1], ['hmm', 'input hmm'], fontsize = 10)\n",
    "plt.xlabel('model', fontsize = 15)\n",
    "plt.ylabel('loglikelihood', fontsize=15)\n",
    "\n",
    "\n",
    "#####               Group-based Analyses - Extract data\n",
    "\n",
    "\n",
    "# Make a file list of data files\n",
    "file_list = glob.glob(f'{base_dir}{func_data}/sub-*nii.gz')\n",
    "\n",
    "# make ROIs\n",
    "target_masker = NiftiMasker(mask_img=target_roi, standardize=False)\n",
    "masker = NiftiLabelsMasker(labels_img=atlas, standardize=False)\n",
    "\n",
    "target_data = {}\n",
    "source_data = {}\n",
    "target_group_data = pd.DataFrame()\n",
    "source_group_data = pd.DataFrame()\n",
    "for f in file_list:  \n",
    "    \n",
    "    sub = f.partition(\"sub-\")[2].rpartition('_task')[0]\n",
    "    # load in data\n",
    "    sdata = image.load_img(f)\n",
    "\n",
    "    # extract target data from roi\n",
    "    target_time_series = target_masker.fit_transform(sdata)\n",
    "    target_data = pd.DataFrame(target_time_series)\n",
    "    target_data['Subject'] = sub\n",
    "    target_group_data = target_group_data.append(target_data)\n",
    "    \n",
    "    # extract source data from rois\n",
    "    source_time_series = masker.fit_transform(sdata)\n",
    "    source_data = pd.DataFrame(zscore(source_time_series))\n",
    "    source_data['Subject'] = sub\n",
    "    source_group_data = source_group_data.append(source_data)\n",
    "    \n",
    "source_group_data.to_csv(os.path.join(out_dir, f'sources_zscoredata.csv'))\n",
    "\n",
    "target_group_data.to_csv(os.path.join(out_dir, f'vmpfc_rawdata.csv'))\n",
    "# Reduce Data Dimensionality\n",
    "target_var = 0.9\n",
    "pca = PCA(n_components=target_var)\n",
    "        \n",
    "X = pd.DataFrame(pca.fit_transform(target_group_data.drop(columns='Subject')))\n",
    "X['Subject'] = target_group_data['Subject'].values\n",
    "X.to_csv(os.path.join(out_dir, f'vmpfc_PCdata.csv'))\n",
    "\n",
    "#####               Group-based Input-driven HMM as in SSM\n",
    "\n",
    "reduced = pd.read_csv(os.path.join(out_dir, f'vmpfc_PCdata.csv'), index_col=0)\n",
    "source = pd.read_csv(os.path.join(out_dir, f'sources_zscoredata.csv'))\n",
    "source.columns = ['Index', 'Amygdala', 'NAcc', 'Hippocampus', 'DLPFC', 'DMPFC', 'pInsula', 'TPJ', 'Subject']\n",
    "\n",
    "# Let's check the correlations between our inputs to the HMM\n",
    "pearsoncorr = source.drop(columns=['Index','Subject']).corr(method='pearson')\n",
    "with sns.plotting_context(context='paper', font_scale=1.5):    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Inputs - Correlation.png', dpi=300)\n",
    "\n",
    "####             Vanilla HMM for group\n",
    "N_iters = 500\n",
    "k = 4 # Chang et al (2021) originally found that 4 states was best solution\n",
    "hmm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], observations=\"diagonal_gaussian\")\n",
    "\n",
    "# Fit\n",
    "hmm_lls = hmm.fit(reduced.drop(columns='Subject').values, method=\"em\", num_iters=N_iters, init_method=\"kmeans\")\n",
    "\n",
    "# Most likely (Viterbi) states\n",
    "hmm_z = hmm.most_likely_states(reduced.drop(columns='Subject').values)\n",
    "\n",
    "# Log Likelihood\n",
    "mle_hmm = hmm.log_likelihood(reduced.drop(columns='Subject').values)\n",
    "bic_hmm = hmm_bic(LL=mle_hmm, n_states=4, n_features=83)\n",
    "\n",
    "#ssm_out = pd.DataFrame()\n",
    "#ssm_out['zPredicted States'] = hmm_z\n",
    "\n",
    "#ssm.to_csv(os.path.join(out_dir,f'SSM-HMMPredictedStates.vmpfc.k{k}.csv'))\n",
    "\n",
    "\n",
    "####             GLM-HMM for group - AMYG and NAcc\n",
    "\n",
    "# First, lets make sure we know which k to use\n",
    "N_iters = 500\n",
    "mle_amygnacc = {}\n",
    "bic_amygnacc = {}\n",
    "for k in tqdm(range(1, 11)):\n",
    "\n",
    "    hmmglm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], source[['Amygdala', 'NAcc']].shape[1], observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "    hmm_amygnacc = hmmglm.fit(reduced.drop(columns='Subject').values, inputs=source[['Amygdala', 'NAcc']].values, method=\"em\", num_iters=N_iters)\n",
    "    mle_amygnacc[k] = hmmglm.log_likelihood(reduced.drop(columns='Subject').values, source[['Amygdala', 'NAcc']].values)\n",
    "    bic_amygnacc[k] = hmm_bic(LL=mle_amygnacc[k], n_states=k, n_features=reduced.drop(columns='Subject').shape[1] + source[['Amygdala', 'NAcc']].shape[1])\n",
    "\n",
    "model_fit = pd.DataFrame(list(bic_amygnacc.items()))\n",
    "model_fit = model_fit.rename(columns={0: \"k\", 1: \"BIC\"})\n",
    "with sns.plotting_context(context='paper', font_scale=2.5):\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    sns.lineplot(data=model_fit, x='k', y='BIC', linewidth=3)\n",
    "    plt.ylabel('Model Fit (BIC)', fontsize=18)\n",
    "    plt.xlabel('k', fontsize=18)\n",
    "    plt.axhline(bic_hmm, color='red', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Model Fit - BIC - AmygdalaNAcc.png', dpi=300)\n",
    "    \n",
    "# Looks like k = 4 is the winner, although it is *just* a bit higher than no input. \n",
    "model_fit.to_csv(os.path.join(out_dir, f'GLM-HMM-AmygNacc-ModelFit.csv'))  \n",
    "\n",
    "#I wonder how Amygdala or NAcc do by themselves\n",
    "\n",
    "# Amygdala\n",
    "k = 4\n",
    "hmmglm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], source[['Amygdala']].shape[1], observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "hmm_amyg = hmmglm.fit(reduced.drop(columns='Subject').values, inputs=source[['Amygdala']].values, method=\"em\", num_iters=N_iters)\n",
    "mle_amyg = hmmglm.log_likelihood(reduced.drop(columns='Subject').values, source[['Amygdala']].values)\n",
    "bic_amyg = hmm_bic(LL=mle_amyg, n_states=k, n_features=reduced.drop(columns='Subject').shape[1] + source[['Amygdala']].shape[1])\n",
    "\n",
    "with sns.plotting_context(context='paper', font_scale=2.5):\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    sns.lineplot(data=model_fit, x='k', y='BIC', linewidth=3)\n",
    "    plt.ylabel('Model Fit (BIC)', fontsize=18)\n",
    "    plt.xlabel('k', fontsize=18)\n",
    "    plt.axhline(bic_hmm, color='red', linestyle='--')\n",
    "    plt.axhline(bic_amyg, color='green', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Model Fit - BIC - AmygdalaNAcc plys Amyg.png', dpi=300)\n",
    "    \n",
    "# NAcc\n",
    "k = 4\n",
    "hmmglm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], source[['NAcc']].shape[1], observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "hmm_nacc = hmmglm.fit(reduced.drop(columns='Subject').values, inputs=source[['NAcc']].values, method=\"em\", num_iters=N_iters)\n",
    "mle_nacc = hmmglm.log_likelihood(reduced.drop(columns='Subject').values, source[['NAcc']].values)\n",
    "bic_nacc = hmm_bic(LL=mle_nacc, n_states=k, n_features=reduced.drop(columns='Subject').shape[1] + source[['NAcc']].shape[1])\n",
    "\n",
    "with sns.plotting_context(context='paper', font_scale=2.5):\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    sns.lineplot(data=model_fit, x='k', y='BIC', linewidth=3)\n",
    "    plt.ylabel('Model Fit (BIC)', fontsize=18)\n",
    "    plt.xlabel('k', fontsize=18)\n",
    "    plt.axhline(bic_hmm, color='red', linestyle='--')\n",
    "    plt.axhline(bic_amyg, color='green', linestyle='--')\n",
    "    plt.axhline(bic_nacc, color='magenta', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Model Fit - BIC - AmygdalaNAcc plus Amyg and NAcc.png', dpi=300)\n",
    "\n",
    "bic_amygnacc = model_fit['BIC'].loc[model_fit['k'] == 4]\n",
    "  \n",
    "with sns.plotting_context(context='paper', font_scale=2):    \n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    loglikelihood_vals = [bic_hmm, bic_amygnacc, bic_amyg, bic_nacc]\n",
    "    colors = ['Red', 'Blue', 'Green', 'Purple']\n",
    "    for z, occ in enumerate(loglikelihood_vals):\n",
    "        plt.bar(z, occ, width = 0.8, color = colors[z])\n",
    "    plt.ylim([13250000, 13290000])\n",
    "    plt.xticks([0, 1,2,3], ['hmm', 'amyg+nacc', 'amyg', 'nacc'], fontsize = 12)\n",
    "    plt.xlabel('model', fontsize = 15)\n",
    "    plt.ylabel('BIC', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Model Fit - BIC - All models k = 4.png', dpi=300)\n",
    "    \n",
    "# collate and save the BICs\n",
    "bics = np.append(bic_amyg, bic_nacc)\n",
    "bics = np.append(bics, bic_amygnacc.values)\n",
    "bics = np.append(bics, bic_hmm)\n",
    "bicspd = pd.DataFrame(bics, columns = ['BICS'])\n",
    "bicspd['models'] = ['Amygdala', 'NAcc', 'Amygdala+Nacc', 'Vanilla', 'Emotions']\n",
    "\n",
    "\n",
    "\n",
    "# Emotion Ratings\n",
    "\n",
    "emotions = pd.read_csv(os.path.join(f'{base_dir}/data/ratings/', f'subjectivityTimeCourse.csv'))\n",
    "emotions = emotions.iloc[::2,:]\n",
    "file_list = glob.glob(f'{base_dir}{func_data}/sub-*nii.gz')\n",
    "emotions_long = pd.concat([emotions]*len(file_list), ignore_index=True)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(emotions_long[['pc1', 'pc2']])\n",
    "\n",
    "k = 4\n",
    "hmmglm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], emotions_long[['pc1', 'pc2']].shape[1], observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "hmm_emotions = hmmglm.fit(reduced.drop(columns='Subject').values, inputs=emotions_long[['pc1', 'pc2']].values, method=\"em\", num_iters=N_iters)\n",
    "mle_emotions = hmmglm.log_likelihood(reduced.drop(columns='Subject').values, emotions_long[['pc1', 'pc2']].values)\n",
    "bic_emotions = hmm_bic(LL=mle_emotions, n_states=k, n_features=reduced.drop(columns='Subject').shape[1] + emotions_long[['pc1', 'pc2']].shape[1])\n",
    "\n",
    "bics = np.append(bics, bic_hmm)\n",
    "bicspd = pd.DataFrame(bics, columns = ['BICS'])\n",
    "bicspd['models'] = ['Amygdala', 'NAcc', 'Amygdala+Nacc', 'Vanilla', 'Emotions']\n",
    "bicspd.to_csv(os.path.join(out_dir, f'GLM-HMM-All-ModelFit.csv')) \n",
    "\n",
    "x = source\n",
    "x[['pc1', 'pc2']] = emotions_long[['pc1', 'pc2']].values\n",
    "pearsoncorr = x.drop(columns=['Index','Subject']).corr(method='pearson')\n",
    "with sns.plotting_context(context='paper', font_scale=1.5):    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Most likely (Viterbi) states\n",
    "    #hmmglm_z = hmmglm.most_likely_states(reduced.drop(columns='Subject').values)\n",
    "    \n",
    "# Maybe orthogonalize Amygdala and NAcc?\n",
    "\n",
    "def gs(X):\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    return Q\n",
    "\n",
    "m = source[['Amygdala', 'NAcc']].values\n",
    "N = gs(m)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(N)\n",
    "r = pearsonr(m[:,0], m[:,1])\n",
    "rN = pearsonr(N[:,0], N[:,1])\n",
    "ortho_source = pd.DataFrame(N)\n",
    "ortho_source.columns = ['Amygdala', 'NAcc_orth']\n",
    "ortho_source[['pc1','pc2']] = emotions_long[['pc1', 'pc2']].values\n",
    "\n",
    "k = 4\n",
    "hmmglm = ssm.HMM(k, reduced.drop(columns='Subject').shape[1], ortho_source[['Amygdala', 'NAcc_orth']].shape[1], observations=\"diagonal_gaussian\", transitions=\"inputdriven\")\n",
    "hmm_amygnacco = hmmglm.fit(reduced.drop(columns='Subject').values, inputs=ortho_source[['Amygdala', 'NAcc_orth']].values, method=\"em\", num_iters=N_iters)\n",
    "mle_amygnacco  = hmmglm.log_likelihood(reduced.drop(columns='Subject').values, ortho_source[['Amygdala', 'NAcc_orth']].values)\n",
    "bic_amygnacco  = hmm_bic(LL=mle_amygnacco , n_states=k, n_features=reduced.drop(columns='Subject').shape[1] + ortho_source[['Amygdala', 'NAcc_orth']].shape[1])\n",
    "\n",
    "bics = np.append(bic_amyg, bic_nacc)\n",
    "bics = np.append(bics, bic_amygnacc.values)\n",
    "bics = np.append(bics, bic_hmm)\n",
    "bics = np.append(bics, bic_emotions)\n",
    "bics = np.append(bics, bic_amygnacco)\n",
    "bicspd = pd.DataFrame(bics, columns = ['BICS'])\n",
    "bicspd['models'] = ['Amygdala', 'NAcc', 'Amygdala+Nacc', 'Vanilla', 'Emotions', 'Amyg+Nacc_ortho+Emotions']\n",
    "\n",
    "with sns.plotting_context(context='paper', font_scale=2):    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    loglikelihood_vals = [bic_hmm, bic_amygnacc, bic_amyg, bic_nacc, bic_emotions, bic_amygnacco]\n",
    "    colors = ['Red', 'Blue', 'Green', 'Magenta', 'Cyan', 'Orange']\n",
    "    for z, occ in enumerate(loglikelihood_vals):\n",
    "        plt.bar(z, occ, width = 0.8, color = colors[z])\n",
    "    plt.ylim([13250000, 13290000])\n",
    "    plt.xticks([0,1,2,3,4,5], ['hmm', 'amyg+nacc', 'amyg', 'nacc', 'emotions', 'a+n_o+e'], fontsize = 12)\n",
    "    plt.xlabel('model', fontsize = 15)\n",
    "    plt.ylabel('BIC', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('Model Fit - BIC - All 5 models k = 4.png', dpi=300)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
